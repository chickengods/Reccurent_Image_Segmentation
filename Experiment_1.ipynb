{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set cuda deivce\n",
    "if t.cuda.is_available():\n",
    "    dev = 'cuda:0'\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "device = t.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngeneral outline\\n\\neach pixel: r, g, b, 8 for prediction, 10 for information flow (21)\\ninput: 9 x pixel, 3x3 convoltuion which is the flattened\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "general outline\n",
    "\n",
    "each pixel: r, g, b, 8 for prediction, 10 for information flow (21)\n",
    "input: 9 x pixel, 3x3 convoltuion which is the flattened (189)\n",
    "1 dense layer: 256 relu\n",
    "output: 8 sigmoid\n",
    "loss: cross entrpoy\n",
    "\n",
    "general traingin methods \n",
    "20 iters of updates the pairwise cross entry of the image\n",
    "\n",
    "model\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceive(state):\n",
    "    sobel_y = np.array([[1,2,1],\n",
    "                        [0,0,0],\n",
    "                        [-1, -2, -1]])\n",
    "    sobel_x = sobel_y.T\n",
    "    \n",
    "    grad_x = cv2.filter2D(state, -1, sobel_x)\n",
    "    grad_y = cv2.filter2D(state, -1, sobel_y)\n",
    "    print(state.shape, grad_x.shape, grad_y.shape)\n",
    "    return np.stack([state, grad_x, grad_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_label(mask):\n",
    "    colors = [(0,0,0), (0,0,1), (0,1,0), (0,1,1), \n",
    "             (1,0,0), (1,0,1), (1,1,0), (1,1,1)]\n",
    "    color_to_index = dict(zip(colors, range(len(colors))))\n",
    "    pix_to_index = lambda pixel: color_to_index.get(tuple([p.item() for p in pixel]))\n",
    "    label_mask = t.zeros(len(colors), mask.shape[1], mask.shape[2])\n",
    "    for i in range(mask.shape[1]):\n",
    "        for j in range(mask.shape[2]):\n",
    "            index = pix_to_index(mask[:,i,j])\n",
    "            if index == None:\n",
    "                print(mask[:,i,j])\n",
    "            label_mask[index, i, j] = 1\n",
    "    return label_mask\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_img(img):\n",
    "    state = t.zeros(27, 128,128)\n",
    "    return t.cat((img, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_mask(pred):\n",
    "    colors = [(0,0,0), (0,0,1), (0,1,0), (0,1,1), \n",
    "             (1,0,0), (1,0,1), (1,1,0), (1,1,1)]\n",
    "    colors = np.array(colors)\n",
    "    return colors[pred.argmax(dim=0)] * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCN(nn.Module):\n",
    "    '''\n",
    "    model for recurrent convoltion network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, kernel_amount, channels_in,\n",
    "                 channels_out, prec_update):\n",
    "        super(RCN, self).__init__()\n",
    "        \n",
    "        self.kernel_amout = kernel_amount\n",
    "        self.channels_out = channels_out\n",
    "        self.prec_udpate = prec_update\n",
    "        self.channels_in = channels_in\n",
    "        \n",
    "        self.preceive = nn.Sequential(\n",
    "            nn.Conv2d(channels_in, kernel_amount, 3, padding='same'),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.chg = nn.Sequential(\n",
    "            nn.Conv2d(kernel_amount, kernel_amount, 1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(kernel_amount, channels_out, 1, padding='same')\n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "    def update(self, x, prec_update=None):\n",
    "        img, state = x[:,:3,:,:], x[:,3:,:,:]\n",
    "        if prec_update == None:\n",
    "            prec_update = self.prec_udpate\n",
    "        ds  = self.chg.forward(self.preceive.forward(x))\n",
    "        update_mask = t.rand(x.shape[-2], x.shape[-1]) <= prec_update\n",
    "        ds *= update_mask\n",
    "        ds = t.cat((t.zeros(1,3,128,128), ds), dim=1)\n",
    "        return x + ds\n",
    "    \n",
    "    def classify(self, x):\n",
    "        return x[:, [-i for i in list(range(1,9))[::-1]],:, :]\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OceanDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_dir = os.path.join(root_dir, 'images')\n",
    "        self.msk_dir = os.path.join(root_dir, 'masks')\n",
    "        self.names = np.array([s.split('.')[0] for s in os.listdir(self.img_dir)])\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.names[idx] + '.jpg')\n",
    "        msk_path = os.path.join(self.msk_dir, self.names[idx] + '.bmp')\n",
    "        img, msk = cv2.imread(img_path), cv2.imread(msk_path)\n",
    "        \n",
    "        #there is no image normalization thus far\n",
    "        #I'm going to see how it works then come back\n",
    "        \n",
    "        #maybe I should be converting the mask into a w x h x label\n",
    "        if self.transform: \n",
    "            img = cv2.cvtColor(cv2.resize(img, (128, 128)), cv2.COLOR_BGR2RGB)\n",
    "            msk = cv2.cvtColor(cv2.resize(msk, (128, 128), interpolation=0), cv2.COLOR_BGR2RGB)\n",
    "            img = self.transform(img)\n",
    "            msk = self.transform(msk)\n",
    "            msk = msk.round()\n",
    "        return img, msk, mask_to_label(msk)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates dataset\n",
    "root_dir = 'data/SUIM/train'\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "od = OceanDataset(root_dir, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formats data into torch tensors\n",
    "os.makedirs('data/SUIM/train/images_trans', exist_ok=True)\n",
    "os.makedirs('data/SUIM/train/masks_pred', exist_ok=True)\n",
    "for i, name in enumerate(od.names):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    img, msk, msk_lb = od[i]\n",
    "    t.save(img, 'data/SUIM/train/images_trans/' + name + '.pt')\n",
    "    t.save(msk_lb, 'data/SUIM/train/masks_pred/' + name + '.pt')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f7aec4e2f10>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcn = RCN(100, 30, 27, .5)\n",
    "img, msk, msk_lb = od[1]\n",
    "x = init_img(img).reshape(1,30,128,128)\n",
    "msk_lb = msk_lb.reshape(1,8,128,128)\n",
    "loss_ce = nn.CrossEntropyLoss()\n",
    "loss_mse = nn.MSELoss()\n",
    "opt = optim.Adam(rcn.parameters(), 0.0001)\n",
    "t.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5883e+11, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    for i in range(20):\n",
    "        x = rcn.update(x)\n",
    "    pred = rcn.classify(x)\n",
    "    loss = loss_mse(pred, msk_lb)\n",
    "    loss.backward()\n",
    "    opt.zero_grad()\n",
    "    opt.step()\n",
    "    x = x.clone().detach()\n",
    "plt.imshow(pred_to_mask(pred[0,:,:,:]))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 268555.6562,  -74868.5312,   53145.2266,  415635.8750, -361666.8750,\n",
       "         334486.2188, -251578.0781, -103061.6641], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0,:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
